{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db837324-5c10-4bef-a0d9-48255199719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 1\n",
    "    \n",
    "Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it. \n",
    "Filter methods are much faster compared to wrapper methods as they do not involve training the models.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe708f4-e994-4acf-91fa-8df092e4463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 2\n",
    "    \n",
    "The main differences between the filter and wrapper methods for feature selection are: \n",
    "    Filter methods measure the relevance of features by their correlation with dependent variable while wrapper \n",
    "    methods measure the usefulness of a subset of feature by actually training a model on it.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3313e3-9f58-4f6b-a89c-bd16b664d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 3\n",
    "    \n",
    "Correlation\n",
    "\n",
    "Backward elimination\n",
    "\n",
    "Filter methods\n",
    "\n",
    "Recursive feature elimination\n",
    "\n",
    "Variance threshold\n",
    "\n",
    "Exhaustive selection\n",
    "\n",
    "Features selection algorithms are as follows\n",
    "\n",
    "Wrapper methods\n",
    "\n",
    "Information gain    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bace60-487a-4ca1-b1ee-e17a96a241e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 4\n",
    "    \n",
    "One downside of such methods is that they do not interact with the predictive model for feature selection.\n",
    "Another disadvantage is seen in the case of univariate filter methods where dependencies between features are \n",
    "normally ignored.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e01ad0-b0f0-4b34-a535-feec335280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 5\n",
    "    \n",
    "\n",
    "For large data you should use the Filter approaches because these approaches are rapid and for small size of data \n",
    "it is better to use Wrapper (KNN, SVM,...) approaches because they are slower than the Filter approaches. \n",
    "or you can combine the two approaches to have better results than the two approaches. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4f1b0-b539-4d45-9cbc-c624746c68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 6\n",
    "    \n",
    "The Filter Method is one approach to feature selection that relies on statistical measures to assess the relevance of features independently of the predictive model being used. Here's how you could use the Filter Method to choose the most pertinent attributes for predicting customer churn in a telecom company:\n",
    "\n",
    "Understand the Dataset: Begin by thoroughly understanding the dataset, including the nature of the features and the target variable (customer churn in this case). This involves examining the data types, distributions, potential correlations, and any domain knowledge that might inform feature selection.\n",
    "\n",
    "Preprocessing: Clean and preprocess the data to handle missing values, outliers, and categorical variables. Standardize or normalize numerical features if necessary.\n",
    "\n",
    "Feature Ranking: Use statistical methods to rank the features based on their relevance to the target variable. Common statistical measures used in the Filter Method include:\n",
    "\n",
    "a. Correlation: Calculate the correlation coefficients between each feature and the target variable (churn). Features with higher absolute correlation coefficients are likely to be more relevant.\n",
    "\n",
    "b. Chi-square Test: For categorical features, perform a chi-square test to evaluate the independence between each feature and the target variable. Features with significant p-values indicate relevance.\n",
    "\n",
    "c. Mutual Information: Compute the mutual information between each feature and the target variable. Mutual information measures the amount of information that one variable contains about another. Higher mutual information suggests higher relevance.\n",
    "\n",
    "Feature Selection: Based on the ranking obtained from the statistical measures, select the top N features to include in the predictive model. You can choose a fixed number of features or set a threshold for feature importance.\n",
    "\n",
    "Model Training and Evaluation: Train the predictive model using the selected features and evaluate its performance on a separate validation or test dataset. Monitor metrics such as accuracy, precision, recall, and F1-score to assess the model's effectiveness in predicting customer churn.\n",
    "\n",
    "Iterative Refinement: Iterate on the feature selection process by experimenting with different sets of features and evaluating their impact on model performance. Fine-tune the feature selection criteria based on the observed results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e85969-4de1-49cb-a82a-51d8be3f367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 7\n",
    "    \n",
    "The Embedded method incorporates feature selection directly into the model training process. It selects the most relevant features by assessing their importance during model training. Here's how you could use the Embedded method to select the most relevant features for predicting the outcome of a soccer match:\n",
    "\n",
    "Choose a Suitable Model: Start by selecting a predictive model that supports feature selection as part of its training process. Ensemble methods like Random Forest and Gradient Boosting, as well as some linear models like Lasso Regression, are commonly used for embedded feature selection.\n",
    "\n",
    "Prepare the Data: Clean and preprocess the dataset, handling missing values, outliers, and encoding categorical variables as necessary. Split the data into training and validation/test sets.\n",
    "\n",
    "Feature Engineering: Create additional features if needed based on domain knowledge or insights from the dataset. This might involve aggregating player statistics, calculating historical performance metrics for teams, or creating interaction terms between features.\n",
    "\n",
    "Model Training with Feature Importance: Train the selected model using the entire set of features. During training, the model automatically assesses the importance of each feature based on how much they contribute to the predictive performance.\n",
    "\n",
    "Feature Importance Evaluation: After training the model, examine the feature importance scores assigned to each feature by the model. Feature importance scores are typically provided by the model itself and indicate the relative importance of each feature in predicting the outcome of soccer matches.\n",
    "\n",
    "Select Relevant Features: Based on the feature importance scores, select the most relevant features for predicting the outcome of soccer matches. You can choose a fixed number of top features or set a threshold for feature importance below which features are discarded.\n",
    "\n",
    "Model Evaluation: Evaluate the performance of the predictive model using the selected features on the validation/test dataset. Monitor metrics such as accuracy, precision, recall, and F1-score to assess the model's effectiveness in predicting soccer match outcomes.\n",
    "\n",
    "Iterative Refinement: Iterate on the feature selection process by experimenting with different sets of features and evaluating their impact on model performance. Fine-tune the model and feature selection criteria based on the observed results.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1eeb76-a496-4ce5-b8d3-4cf3e4849b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Answer : 8\n",
    "    \n",
    "The Wrapper method for feature selection involves selecting subsets of features and evaluating their performance using a specific machine learning algorithm. Here's how you could use the Wrapper method to select the best set of features for predicting the price of a house:\n",
    "\n",
    "Define Candidate Feature Subsets: Begin by creating different subsets of features from your dataset. You can start with subsets containing individual features and progressively combine them to form larger feature sets.\n",
    "\n",
    "Choose a Performance Metric: Select a performance metric to evaluate the predictive performance of each feature subset. Common metrics for regression tasks, such as predicting house prices, include mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE).\n",
    "\n",
    "Select a Machine Learning Algorithm: Choose a machine learning algorithm that is suitable for regression tasks. Popular choices include linear regression, decision trees, random forests, gradient boosting, or support vector machines.\n",
    "\n",
    "Apply Cross-Validation: Split your dataset into training and validation sets. Apply cross-validation techniques, such as k-fold cross-validation, to ensure robustness in the evaluation of different feature subsets.\n",
    "\n",
    "Feature Subset Evaluation:\n",
    "\n",
    "Train the chosen machine learning algorithm on each feature subset.\n",
    "Evaluate the predictive performance of each model using the selected performance metric.\n",
    "Keep track of the performance metric for each feature subset.\n",
    "Select the Best Feature Subset:\n",
    "\n",
    "Identify the feature subset that achieves the best performance metric.\n",
    "This could be the subset with the lowest MSE, RMSE, or MAE, depending on your chosen performance metric.\n",
    "Alternatively, you can use other criteria such as simplicity or interpretability to choose the best subset if multiple subsets perform similarly in terms of predictive accuracy.\n",
    "Validate the Selected Subset: Once you've identified the best feature subset using the training/validation data, validate its performance on a separate test dataset. This ensures that the selected subset generalizes well to unseen data.\n",
    "\n",
    "Iterative Refinement: If necessary, repeat the process with different combinations of features or fine-tune the hyperparameters of the machine learning algorithm to further optimize performance.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
